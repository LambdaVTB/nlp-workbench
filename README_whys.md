# Почемучки или почему система такая какая есть

## Оглавление

## Почему использовали именно Natasha?

Авторы этих моделей специально проводили квантизацию чтобы ускорить модель в несколько раз и в итоге все необходимые веса в сумме занимают менее 100Мб. Более того это значительно ускоряет саму модель, позволяя обрабатывать большое количество потоков информации даже на не высокопроизводительных серверах.

Модели, представленные в данной библиотеки были обучены на корпусе из русских новостей, что соответствует нашей задаче. "Для новостных статей качество на всех задачах сравнимо или превосходит существующие решения. Библиотека поддерживает Python 3.5+ и PyPy3, не требует GPU, зависит только от NumPy. " - утверждают авторы на главной странице.

Более того, решение имеет подробную документацию и имеют активное комьюнити разработчиков.

## Определение дайджеста - [scripts/rss_parser.py](scripts/rss_parser.py)

Сайты с RSS-фидами —> Ежедневное обновление последних новостей

В результате формируется локальная база новостей —> Новости за последнюю неделю —> Борьба с дубликатами —> Фильтрация по тегам (каждая новость имеет тег, определяющий тематику, набор тематик для роли захардкожен) —> Сортировка по дате + рекомендации от рекомендательной системы, реализованной на графах по последнему месяцу.

По функционалу схоже с дайджестом новостей на yandex.ru

## Рекомендательная система

Рекомендательная система реализована на графах. По каждой новости строится граф, в котором вершины — это пользователи, а ребра — это заход пользователя на новость. По графу строится  представление новости. По этому представлению строится рекомендация. Подробную реализацию можно посмотреть в репозитории [бекенда](https://github.com/LambdaVTB/backend-api-employee/blob/main/service/endpoints/news.py).

## Суммирование дубликатов

При обработке входящего потока новостей, новые новости сравниваются с уже имеющимися в базе. Если новость уже есть в базе, то теги новостей конкатенируются, ссылки на источники тоже. Оставляется первый заголовок и дата. Таким образом, две или более одинаковые новости сливаются в одну.

Определение является новость дубликатом или нет происходит на [бекенде](https://github.com/LambdaVTB/backend-api-employee).

Являются новости дубликатами, или нет, определяется на основе заголовка. Иползуется полнотекстовый поиск по базе новостей для поиска кандидатов для сравнения. После этого происходит сравнение заголовков. Если заголовки совпадают, то новость считается дубликатом. Если нет, то новость не считается дубликатом. В более редких случаях
дубликатов, новости отличаются совсем немного, поэтому заголовки, между которыми расстояние Левенштейна невелико, считаются дубликатами.

## Определение трендов - [scripts/get_trends.py](scripts/get_trends.py)

1. Возможность определять общие тренды - тренды по всем новостям - `get_trends`

Набор новостей за последние n дней —> Из заголовков выделяем Named Entities (названия и имена), и отдельно другие существительные  —> Считаем кол-во NE, получаем самые упоминаемые, а значит популярные места (LOCs), людей (PERs), организации (ORGs), за последнее время  —> это и есть тренд

2. Возможность определять тренды по ролям - профильные тренды - `get_trends_by_tag`

Набор новостей за последние n дней —> Из заголовков выделяем Named Entities (названия и имена), и отдельно другие существительные  —> Считаем кол-во NE и существиетлных, получаем самые упоминаемые, а значит популярные места (LOCs), людей (PERs), организации (ORGs), и вещи (NOUs) за последнее время  —> это и есть тренд

## Определение инсайтов - [scripts/insight_miner.py](scripts/insight_miner.py)

1. Инсайт - Аналитика и полезные материалы

Тренды по тегам —> Поиск статей на vc.ru, habr.com, других блогоподобных площадках, где люди высказывают аналитику и свое мнение —> Полезные материалы

2. Инсайт - Мнение общества (WIP/Planned)

Тренды по тегам —> Поиск упоминаний этого слова в твиттере (через поиск твиттера) —> Конкатенация всех мнений —> Суммаризация мнений с помощью нейронной сети —> Короткий текст, отражающий отношение людей к тренду

### Почему twitter?

- Международная платформа
- Широкие возможности для фильтрации по кол-ву лайков \ языку \ отклику людей \ региону \ тд тп
- Можно выделить список аккаунтов-авторитетов \ лидеров мнений
- Можно выделить подтвержденные аккаунты как в целом формулирующие мнение
- Здесь люди стремятся высказать свое мнение, поучаствовать в дискуссии, при этом посты сильно связаны друг с другом

### Почему WIP?

- Сложно подключиться
- Отсутствует API и поэтому трудности в парсинге (!!!)
- Однако проблемы нивелируются, если получить twitter developer аккаунт
- Пайплайн рабочий, если удается получить твиты и дает хорошие результаты, но пока не реализовано из-за отсутствия API

## Отбор источников

При выборе метода получения информации мы руководствовались тем, что хотел получить масштабируемую систему с максимальной простотой интеграции новых источников.

Как следствие - наш выбор пал на источники, предоставляющие каналы RSS. Это стандартизирован канал новостных блогов, поддерживающий теги.

Стоит отметить, что не все источники имеют теги. В дальнейшем развитии системы для решения этой проблемы можно обучить нейросеть, которая бы определяла к какому  тегу относится новость. (у нас не реализовано)

Поскольку нам было необходимо продемонстрировать работоспособность системы и возможность персонализированных рекомендаций по ролям - мы выбрали источники про бизнес, финансы, бухгалтерский учет, и несколько общих. Такой выбор позволяет проверить работоспособность системы при пересекающихся тегах у ролей, при не пересекающихся.

В качестве данных для заполнения базы данных использовали данные лента.ру за 2 года (<https://www.kaggle.com/datasets/yutkin/corpus-of-russian-news-articles-from-lenta>)
