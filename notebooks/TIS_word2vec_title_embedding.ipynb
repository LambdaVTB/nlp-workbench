{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from navec import Navec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"S:\\Workspace\\MORETECH\\nlp-workbench\\models\\navec_news_v1_1B_250K_300d_100q.tar\"\n",
    "navec = Navec.load(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch\n",
    "# %pip install torchtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from slovnet.model.emb import NavecEmbedding\n",
    "\n",
    "# navec_emb = NavecEmbedding(navec)\n",
    "# input = torch.tensor([1,2,0])\n",
    "# output = navec_emb(input)\n",
    "\n",
    "\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install sent2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "from sent2vec.vectorizer import Vectorizer\n",
    "\n",
    "vectorizer = Vectorizer(pretrained_weights=\"DeepPavlov/rubert-base-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Рэпер Паша Техник впал в кому после отдыха на вписке с наркотиками\",\n",
    "    \"Рэпер Паша Техник впал в кому после вечеринки с наркотиками\",\n",
    "    \"Я не люблю кошек\",\n",
    "]\n",
    "\n",
    "vectorizer.run(sentences)\n",
    "vectors = vectorizer.vectors[-3:]\n",
    "dist_1 = spatial.distance.cosine(vectors[0], vectors[1])\n",
    "dist_2 = spatial.distance.cosine(vectors[0], vectors[2])\n",
    "print('dist_1: {0}, dist_2: {1}'.format(dist_1, dist_2))\n",
    "assert dist_1 < dist_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! inference speed is proportional to len(n)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 23.709993600845337 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test inference speed for 100 sentences\n",
    "import time\n",
    "start = time.time()\n",
    "vectorizer.run(sentences * 1000)\n",
    "end = time.time()\n",
    "print('Inference time: {0} seconds'.format(end - start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorizer_func(vectorizer):\n",
    "    def func(sentences):\n",
    "        vectorizer.run([sentences])\n",
    "        return vectorizer.vectors[-1]\n",
    "    return func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414f5aac00db483db9829b1257c17524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=750), Label(value='0 / 750'))), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 67.2250063419342 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test inference speed for 100 sentences, but make it parallel\n",
    "import time\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=4)\n",
    "\n",
    "\n",
    "# Create a dataframe with 100 sentences\n",
    "vectorize_func = get_vectorizer_func(vectorizer)\n",
    "data = sentences * 1000\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Apply vectorize_func to each sentence in parallel\n",
    "df = pd.DataFrame(data, columns=['sentence'])\n",
    "df['vector'] = df['sentence'].parallel_apply(vectorize_func)\n",
    "\n",
    "end = time.time()\n",
    "print('Inference time: {0} seconds'.format(end - start))\n",
    "\n",
    "# Модель для каждого потока грузится в память - memory consumption 2gb * n_workers,\n",
    "# После этого инференс относительно быстро производится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# За 4 минуты делается инференс на 10000 новостей (при распаралелливании)\n",
    "# Походу в модели есть свое распараллеливание или она просто крутая, \n",
    "# три тысячи записей она делает сама за 23 секунды\n",
    "# Предположение, все плохо работает из за рейс кондишена в vectorizer... или нет\n",
    "# В общем без распараллеливания нормально\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Bert sberbank-ai/sbert_large_nlu_ru\n",
      "Vectorization done on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n",
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at sberbank-ai/sbert_large_nlu_ru were not used when initializing DistilBertModel: ['encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.17.output.dense.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.23.output.dense.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.19.output.dense.bias', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.13.output.dense.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.12.output.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.21.output.dense.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.16.output.dense.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.23.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.22.output.dense.bias', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.18.output.dense.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.self.key.weight', 'embeddings.position_ids', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.20.output.dense.bias', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.14.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.16.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.19.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.21.output.dense.bias', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.15.output.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.15.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.12.output.dense.bias', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.17.output.dense.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.16.intermediate.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.22.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.19.output.dense.weight', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.17.attention.self.value.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertModel were not initialized from the model checkpoint at sberbank-ai/sbert_large_nlu_ru and are newly initialized: ['transformer.layer.23.attention.q_lin.bias', 'transformer.layer.15.attention.q_lin.bias', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.23.ffn.lin1.bias', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.18.attention.q_lin.bias', 'transformer.layer.21.attention.q_lin.bias', 'transformer.layer.13.ffn.lin1.weight', 'transformer.layer.12.attention.q_lin.weight', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.17.sa_layer_norm.bias', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.23.output_layer_norm.bias', 'transformer.layer.17.attention.k_lin.bias', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.13.attention.out_lin.bias', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.13.attention.out_lin.weight', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.13.sa_layer_norm.weight', 'transformer.layer.18.output_layer_norm.weight', 'transformer.layer.22.sa_layer_norm.weight', 'transformer.layer.13.ffn.lin2.bias', 'transformer.layer.23.attention.out_lin.bias', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.19.ffn.lin2.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.22.output_layer_norm.bias', 'transformer.layer.21.attention.v_lin.bias', 'transformer.layer.19.attention.v_lin.weight', 'transformer.layer.13.attention.k_lin.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.8.sa_layer_norm.weight', 'transformer.layer.18.attention.q_lin.weight', 'transformer.layer.21.attention.v_lin.weight', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.22.attention.q_lin.bias', 'transformer.layer.18.attention.v_lin.bias', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.13.attention.k_lin.bias', 'transformer.layer.18.attention.v_lin.weight', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.12.attention.out_lin.weight', 'transformer.layer.15.attention.v_lin.bias', 'transformer.layer.12.attention.k_lin.bias', 'transformer.layer.15.attention.out_lin.bias', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.16.attention.v_lin.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.15.ffn.lin1.bias', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.12.ffn.lin1.bias', 'transformer.layer.22.ffn.lin1.weight', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.20.ffn.lin1.bias', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.13.ffn.lin2.weight', 'transformer.layer.17.attention.v_lin.bias', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.7.ffn.lin1.bias', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.15.ffn.lin2.weight', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.20.sa_layer_norm.bias', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.11.output_layer_norm.bias', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.20.attention.q_lin.bias', 'transformer.layer.19.attention.v_lin.bias', 'transformer.layer.20.attention.k_lin.weight', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.17.attention.out_lin.weight', 'transformer.layer.13.output_layer_norm.bias', 'transformer.layer.20.attention.out_lin.weight', 'transformer.layer.23.sa_layer_norm.bias', 'transformer.layer.23.attention.k_lin.weight', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.20.attention.q_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.19.attention.q_lin.weight', 'transformer.layer.18.attention.k_lin.weight', 'transformer.layer.14.output_layer_norm.weight', 'transformer.layer.14.attention.q_lin.weight', 'transformer.layer.16.attention.v_lin.weight', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.19.output_layer_norm.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.22.attention.k_lin.bias', 'transformer.layer.23.attention.k_lin.bias', 'transformer.layer.17.output_layer_norm.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.13.attention.v_lin.bias', 'transformer.layer.12.attention.k_lin.weight', 'transformer.layer.19.sa_layer_norm.weight', 'transformer.layer.21.attention.out_lin.weight', 'transformer.layer.15.sa_layer_norm.weight', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.14.attention.k_lin.weight', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.14.output_layer_norm.bias', 'transformer.layer.22.attention.v_lin.weight', 'transformer.layer.14.ffn.lin2.weight', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.12.attention.v_lin.bias', 'transformer.layer.21.output_layer_norm.weight', 'transformer.layer.15.sa_layer_norm.bias', 'transformer.layer.14.attention.out_lin.weight', 'transformer.layer.20.output_layer_norm.weight', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.19.sa_layer_norm.bias', 'transformer.layer.17.attention.q_lin.weight', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.21.attention.k_lin.weight', 'transformer.layer.18.sa_layer_norm.weight', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.19.attention.k_lin.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.15.ffn.lin2.bias', 'transformer.layer.23.ffn.lin2.bias', 'transformer.layer.14.attention.k_lin.bias', 'transformer.layer.9.output_layer_norm.weight', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.21.ffn.lin1.weight', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.13.ffn.lin1.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.17.ffn.lin2.bias', 'transformer.layer.20.attention.v_lin.bias', 'transformer.layer.19.attention.out_lin.weight', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.20.attention.v_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.14.attention.v_lin.weight', 'transformer.layer.20.sa_layer_norm.weight', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.19.attention.q_lin.bias', 'transformer.layer.20.ffn.lin2.weight', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.16.attention.k_lin.weight', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.21.sa_layer_norm.bias', 'transformer.layer.12.output_layer_norm.bias', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.17.ffn.lin2.weight', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.16.ffn.lin1.bias', 'transformer.layer.17.ffn.lin1.bias', 'transformer.layer.19.attention.out_lin.bias', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.12.output_layer_norm.weight', 'transformer.layer.14.ffn.lin1.bias', 'transformer.layer.13.output_layer_norm.weight', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.22.attention.out_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.16.attention.q_lin.bias', 'transformer.layer.17.ffn.lin1.weight', 'transformer.layer.16.attention.q_lin.weight', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.17.attention.k_lin.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.12.sa_layer_norm.weight', 'transformer.layer.21.ffn.lin2.weight', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.13.attention.q_lin.weight', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.18.ffn.lin2.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.14.attention.q_lin.bias', 'transformer.layer.13.attention.q_lin.bias', 'transformer.layer.16.output_layer_norm.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.13.sa_layer_norm.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.22.sa_layer_norm.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.17.attention.out_lin.bias', 'transformer.layer.6.attention.q_lin.weight', 'transformer.layer.14.ffn.lin2.bias', 'transformer.layer.12.ffn.lin1.weight', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.12.ffn.lin2.bias', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.21.sa_layer_norm.weight', 'transformer.layer.21.ffn.lin2.bias', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.16.ffn.lin2.weight', 'transformer.layer.22.ffn.lin1.bias', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.15.output_layer_norm.weight', 'transformer.layer.14.attention.out_lin.bias', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.22.attention.v_lin.bias', 'transformer.layer.22.attention.out_lin.weight', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.22.attention.k_lin.weight', 'transformer.layer.15.attention.k_lin.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.23.output_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.17.attention.v_lin.weight', 'transformer.layer.14.sa_layer_norm.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.16.sa_layer_norm.weight', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.19.ffn.lin1.bias', 'transformer.layer.15.ffn.lin1.weight', 'transformer.layer.19.ffn.lin2.weight', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.16.ffn.lin1.weight', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.20.attention.out_lin.bias', 'transformer.layer.23.attention.q_lin.weight', 'transformer.layer.23.sa_layer_norm.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.12.sa_layer_norm.bias', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.15.attention.k_lin.bias', 'transformer.layer.17.attention.q_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.20.ffn.lin1.weight', 'transformer.layer.20.ffn.lin2.bias', 'transformer.layer.14.attention.v_lin.bias', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.17.sa_layer_norm.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.11.attention.v_lin.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.12.attention.q_lin.bias', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.23.ffn.lin2.weight', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.14.sa_layer_norm.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.16.output_layer_norm.weight', 'transformer.layer.16.ffn.lin2.bias', 'transformer.layer.18.ffn.lin2.bias', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.20.attention.k_lin.bias', 'transformer.layer.23.attention.out_lin.weight', 'transformer.layer.15.attention.q_lin.weight', 'transformer.layer.18.attention.k_lin.bias', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.16.attention.k_lin.bias', 'transformer.layer.23.attention.v_lin.weight', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.22.ffn.lin2.weight', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.16.attention.out_lin.bias', 'transformer.layer.12.attention.v_lin.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.23.ffn.lin1.weight', 'transformer.layer.18.sa_layer_norm.bias', 'transformer.layer.22.output_layer_norm.weight', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.19.ffn.lin1.weight', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.22.ffn.lin2.bias', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.21.attention.q_lin.weight', 'transformer.layer.18.attention.out_lin.weight', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.21.attention.k_lin.bias', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.21.output_layer_norm.bias', 'transformer.layer.12.ffn.lin2.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.19.attention.k_lin.weight', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.17.output_layer_norm.weight', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.18.attention.out_lin.bias', 'transformer.layer.15.output_layer_norm.bias', 'transformer.layer.18.ffn.lin1.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.23.attention.v_lin.bias', 'transformer.layer.18.ffn.lin1.weight', 'transformer.layer.15.attention.v_lin.weight', 'transformer.layer.16.sa_layer_norm.bias', 'transformer.layer.16.attention.out_lin.weight', 'transformer.layer.15.attention.out_lin.weight', 'transformer.layer.20.output_layer_norm.bias', 'transformer.layer.19.output_layer_norm.bias', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.18.output_layer_norm.bias', 'transformer.layer.21.attention.out_lin.bias', 'transformer.layer.21.ffn.lin1.bias', 'transformer.layer.13.attention.v_lin.weight', 'transformer.layer.14.ffn.lin1.weight', 'transformer.layer.22.attention.q_lin.weight', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.12.attention.out_lin.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "0.1140473484992981\n",
      "================================================================================\n",
      "[0.06608426570892334,\n",
      " 0.29877471923828125,\n",
      " 0.2968018651008606,\n",
      " 0.31692326068878174,\n",
      " 0.28476405143737793,\n",
      " 0.2781725525856018,\n",
      " 0.17978978157043457,\n",
      " 0.2909168601036072,\n",
      " 0.21751892566680908,\n",
      " 0.2092221975326538]\n",
      "================================================================================\n",
      "[0, 0.06608426570892334, 0.29877471923828125, 0.2968018651008606, 0.31692326068878174, 0.28476405143737793, 0.2781725525856018, 0.17978978157043457, 0.2909168601036072, 0.21751892566680908, 0.2092221975326538]\n",
      "[0.06608426570892334, 0, 0.16427701711654663, 0.16348499059677124, 0.180597186088562, 0.15382540225982666, 0.14378350973129272, 0.12244385480880737, 0.39273422956466675, 0.11091035604476929, 0.12085115909576416]\n",
      "[0.29877471923828125, 0.16427701711654663, 0, 0.0037273168563842773, 0.012334585189819336, 0.027603864669799805, 0.02644813060760498, 0.15048104524612427, 0.5028277039527893, 0.07137799263000488, 0.12648940086364746]\n",
      "[0.2968018651008606, 0.16348499059677124, 0.0037273168563842773, 0, 0.008241653442382812, 0.02700483798980713, 0.028919100761413574, 0.14995664358139038, 0.5072855651378632, 0.07257574796676636, 0.12513840198516846]\n",
      "[0.31692326068878174, 0.180597186088562, 0.012334585189819336, 0.008241653442382812, 0, 0.021843552589416504, 0.02942633628845215, 0.1578187346458435, 0.5251553356647491, 0.07043761014938354, 0.1285904049873352]\n",
      "[0.28476405143737793, 0.15382540225982666, 0.027603864669799805, 0.02700483798980713, 0.021843552589416504, 0, 0.03674858808517456, 0.15231424570083618, 0.5173734128475189, 0.06645321846008301, 0.12964481115341187]\n",
      "[0.2781725525856018, 0.14378350973129272, 0.02644813060760498, 0.028919100761413574, 0.02942633628845215, 0.03674858808517456, 0, 0.11122530698776245, 0.48055481910705566, 0.04596257209777832, 0.10588669776916504]\n",
      "[0.17978978157043457, 0.12244385480880737, 0.15048104524612427, 0.14995664358139038, 0.1578187346458435, 0.15231424570083618, 0.11122530698776245, 0, 0.28924280405044556, 0.07458066940307617, 0.03874844312667847]\n",
      "[0.2909168601036072, 0.39273422956466675, 0.5028277039527893, 0.5072855651378632, 0.5251553356647491, 0.5173734128475189, 0.48055481910705566, 0.28924280405044556, 0, 0.40715062618255615, 0.3677886128425598]\n",
      "[0.21751892566680908, 0.11091035604476929, 0.07137799263000488, 0.07257574796676636, 0.07043761014938354, 0.06645321846008301, 0.04596257209777832, 0.07458066940307617, 0.40715062618255615, 0, 0.06389421224594116]\n",
      "[0.2092221975326538, 0.12085115909576416, 0.12648940086364746, 0.12513840198516846, 0.1285904049873352, 0.12964481115341187, 0.10588669776916504, 0.03874844312667847, 0.3677886128425598, 0.06389421224594116, 0]\n",
      "                                                    Рэпер Паша Техник впал в кому после отдыха на вписке с наркотиками  \\\n",
      "Рэпер Паша Техник впал в кому после отдыха на в...                                               True                    \n",
      "Рэпер Паша Техник впал в кому после вечеринки с...                                               True                    \n",
      "Я не люблю кошек                                                                                False                    \n",
      "Я не люблю собак                                                                                False                    \n",
      "Я боюсь собак                                                                                   False                    \n",
      "Я боюсь Пашу Техника                                                                            False                    \n",
      "Путин отмечает 70-летие                                                                         False                    \n",
      "Владимир Путин отмечает юбилей: президенту Росс...                                              False                    \n",
      "Владимиру Путину — 70 лет! Топ-5 интересных фак...                                              False                    \n",
      "Владимиру Путину исполнилось 70 лет                                                             False                    \n",
      "День рождения Путина: президенту России исполни...                                              False                    \n",
      "\n",
      "                                                    Рэпер Паша Техник впал в кому после вечеринки с наркотиками  \\\n",
      "Рэпер Паша Техник впал в кому после отдыха на в...                                               True             \n",
      "Рэпер Паша Техник впал в кому после вечеринки с...                                               True             \n",
      "Я не люблю кошек                                                                                False             \n",
      "Я не люблю собак                                                                                False             \n",
      "Я боюсь собак                                                                                   False             \n",
      "Я боюсь Пашу Техника                                                                            False             \n",
      "Путин отмечает 70-летие                                                                         False             \n",
      "Владимир Путин отмечает юбилей: президенту Росс...                                              False             \n",
      "Владимиру Путину — 70 лет! Топ-5 интересных фак...                                              False             \n",
      "Владимиру Путину исполнилось 70 лет                                                             False             \n",
      "День рождения Путина: президенту России исполни...                                              False             \n",
      "\n",
      "                                                    Я не люблю кошек  \\\n",
      "Рэпер Паша Техник впал в кому после отдыха на в...             False   \n",
      "Рэпер Паша Техник впал в кому после вечеринки с...             False   \n",
      "Я не люблю кошек                                                True   \n",
      "Я не люблю собак                                                True   \n",
      "Я боюсь собак                                                   True   \n",
      "Я боюсь Пашу Техника                                            True   \n",
      "Путин отмечает 70-летие                                         True   \n",
      "Владимир Путин отмечает юбилей: президенту Росс...             False   \n",
      "Владимиру Путину — 70 лет! Топ-5 интересных фак...             False   \n",
      "Владимиру Путину исполнилось 70 лет                             True   \n",
      "День рождения Путина: президенту России исполни...             False   \n",
      "\n",
      "                                                    Я не люблю собак  \\\n",
      "Рэпер Паша Техник впал в кому после отдыха на в...             False   \n",
      "Рэпер Паша Техник впал в кому после вечеринки с...             False   \n",
      "Я не люблю кошек                                                True   \n",
      "Я не люблю собак                                                True   \n",
      "Я боюсь собак                                                   True   \n",
      "Я боюсь Пашу Техника                                            True   \n",
      "Путин отмечает 70-летие                                         True   \n",
      "Владимир Путин отмечает юбилей: президенту Росс...             False   \n",
      "Владимиру Путину — 70 лет! Топ-5 интересных фак...             False   \n",
      "Владимиру Путину исполнилось 70 лет                             True   \n",
      "День рождения Путина: президенту России исполни...             False   \n",
      "\n",
      "                                                    Я боюсь собак  \\\n",
      "Рэпер Паша Техник впал в кому после отдыха на в...          False   \n",
      "Рэпер Паша Техник впал в кому после вечеринки с...          False   \n",
      "Я не люблю кошек                                             True   \n",
      "Я не люблю собак                                             True   \n",
      "Я боюсь собак                                                True   \n",
      "Я боюсь Пашу Техника                                         True   \n",
      "Путин отмечает 70-летие                                      True   \n",
      "Владимир Путин отмечает юбилей: президенту Росс...          False   \n",
      "Владимиру Путину — 70 лет! Топ-5 интересных фак...          False   \n",
      "Владимиру Путину исполнилось 70 лет                          True   \n",
      "День рождения Путина: президенту России исполни...          False   \n",
      "\n",
      "                                                    Я боюсь Пашу Техника  \\\n",
      "Рэпер Паша Техник впал в кому после отдыха на в...                 False   \n",
      "Рэпер Паша Техник впал в кому после вечеринки с...                 False   \n",
      "Я не люблю кошек                                                    True   \n",
      "Я не люблю собак                                                    True   \n",
      "Я боюсь собак                                                       True   \n",
      "Я боюсь Пашу Техника                                                True   \n",
      "Путин отмечает 70-летие                                             True   \n",
      "Владимир Путин отмечает юбилей: президенту Росс...                 False   \n",
      "Владимиру Путину — 70 лет! Топ-5 интересных фак...                 False   \n",
      "Владимиру Путину исполнилось 70 лет                                 True   \n",
      "День рождения Путина: президенту России исполни...                 False   \n",
      "\n",
      "                                                    Путин отмечает 70-летие  \\\n",
      "Рэпер Паша Техник впал в кому после отдыха на в...                    False   \n",
      "Рэпер Паша Техник впал в кому после вечеринки с...                    False   \n",
      "Я не люблю кошек                                                       True   \n",
      "Я не люблю собак                                                       True   \n",
      "Я боюсь собак                                                          True   \n",
      "Я боюсь Пашу Техника                                                   True   \n",
      "Путин отмечает 70-летие                                                True   \n",
      "Владимир Путин отмечает юбилей: президенту Росс...                    False   \n",
      "Владимиру Путину — 70 лет! Топ-5 интересных фак...                    False   \n",
      "Владимиру Путину исполнилось 70 лет                                    True   \n",
      "День рождения Путина: президенту России исполни...                    False   \n",
      "\n",
      "                                                    Владимир Путин отмечает юбилей: президенту России исполнилось 70 лет  \\\n",
      "Рэпер Паша Техник впал в кому после отдыха на в...                                              False                      \n",
      "Рэпер Паша Техник впал в кому после вечеринки с...                                              False                      \n",
      "Я не люблю кошек                                                                                False                      \n",
      "Я не люблю собак                                                                                False                      \n",
      "Я боюсь собак                                                                                   False                      \n",
      "Я боюсь Пашу Техника                                                                            False                      \n",
      "Путин отмечает 70-летие                                                                         False                      \n",
      "Владимир Путин отмечает юбилей: президенту Росс...                                               True                      \n",
      "Владимиру Путину — 70 лет! Топ-5 интересных фак...                                              False                      \n",
      "Владимиру Путину исполнилось 70 лет                                                              True                      \n",
      "День рождения Путина: президенту России исполни...                                               True                      \n",
      "\n",
      "                                                    Владимиру Путину — 70 лет! Топ-5 интересных фактов о жизни президента России  \\\n",
      "Рэпер Паша Техник впал в кому после отдыха на в...                                              False                              \n",
      "Рэпер Паша Техник впал в кому после вечеринки с...                                              False                              \n",
      "Я не люблю кошек                                                                                False                              \n",
      "Я не люблю собак                                                                                False                              \n",
      "Я боюсь собак                                                                                   False                              \n",
      "Я боюсь Пашу Техника                                                                            False                              \n",
      "Путин отмечает 70-летие                                                                         False                              \n",
      "Владимир Путин отмечает юбилей: президенту Росс...                                              False                              \n",
      "Владимиру Путину — 70 лет! Топ-5 интересных фак...                                               True                              \n",
      "Владимиру Путину исполнилось 70 лет                                                             False                              \n",
      "День рождения Путина: президенту России исполни...                                              False                              \n",
      "\n",
      "                                                    Владимиру Путину исполнилось 70 лет  \\\n",
      "Рэпер Паша Техник впал в кому после отдыха на в...                                False   \n",
      "Рэпер Паша Техник впал в кому после вечеринки с...                                False   \n",
      "Я не люблю кошек                                                                   True   \n",
      "Я не люблю собак                                                                   True   \n",
      "Я боюсь собак                                                                      True   \n",
      "Я боюсь Пашу Техника                                                               True   \n",
      "Путин отмечает 70-летие                                                            True   \n",
      "Владимир Путин отмечает юбилей: президенту Росс...                                 True   \n",
      "Владимиру Путину — 70 лет! Топ-5 интересных фак...                                False   \n",
      "Владимиру Путину исполнилось 70 лет                                                True   \n",
      "День рождения Путина: президенту России исполни...                                 True   \n",
      "\n",
      "                                                    День рождения Путина: президенту России исполнилось 70 лет  \n",
      "Рэпер Паша Техник впал в кому после отдыха на в...                                              False           \n",
      "Рэпер Паша Техник впал в кому после вечеринки с...                                              False           \n",
      "Я не люблю кошек                                                                                False           \n",
      "Я не люблю собак                                                                                False           \n",
      "Я боюсь собак                                                                                   False           \n",
      "Я боюсь Пашу Техника                                                                            False           \n",
      "Путин отмечает 70-летие                                                                         False           \n",
      "Владимир Путин отмечает юбилей: президенту Росс...                                               True           \n",
      "Владимиру Путину — 70 лет! Топ-5 интересных фак...                                              False           \n",
      "Владимиру Путину исполнилось 70 лет                                                              True           \n",
      "День рождения Путина: президенту России исполни...                                               True           \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from turtle import distance\n",
    "from scipy import spatial\n",
    "from sent2vec.vectorizer import Vectorizer\n",
    "\n",
    "\n",
    "\n",
    "class SimilarityModel:\n",
    "    def __init__(self, weights: str = \"DeepPavlov/rubert-base-cased\" ) -> None:\n",
    "        self.vectorizer = Vectorizer(weights)\n",
    "\n",
    "    def calc_embeddings(self, sentences: list[str]) -> list:\n",
    "        # Outputs embeddings for each sentence in the list\n",
    "        # Efficient for batch processing\n",
    "        vectors = []\n",
    "        self.vectorizer.run(sentences)\n",
    "        vectors = self.vectorizer.vectors[-len(sentences):]\n",
    "        return self.vectors\n",
    "\n",
    "    def calc_distance_matrix_from_embeddings(self, embeddings1: list, embeddings2: list) -> list:\n",
    "        distances = []\n",
    "        for vector in embeddings1:\n",
    "            distances.append([])\n",
    "            for vector2 in embeddings2:\n",
    "                distances[-1].append(spatial.distance.cosine(vector, vector2))\n",
    "        return distances\n",
    "\n",
    "\n",
    "    def calc_distance(self, sentence1: str, sentence2: str) -> float:\n",
    "        # USE FOR TESTS ONLY\n",
    "        # IT IS UNIMAGINABLY SLOW IN PRODUCTION\n",
    "        # USE LATER calc_similarity_batch\n",
    "        self.vectorizer.run([sentence1, sentence2])\n",
    "        vectors = self.vectorizer.vectors[-2:]\n",
    "        return spatial.distance.cosine(vectors[0] / len(sentence1), vectors[1] / len(sentence2))\n",
    "\n",
    "    def calc_distance_batch(self, sentence1: str, sentences: list[str]) -> list:\n",
    "        self.vectorizer.run([sentence1] + sentences)\n",
    "        vectors = self.vectorizer.vectors[-(len(sentences) + 1):]\n",
    "        distances = []\n",
    "        for vector in vectors[1:]:\n",
    "            distances.append(spatial.distance.cosine(vectors[0], vector))\n",
    "        return distances\n",
    "\n",
    "    def calc_distance_matrix(self, sentences: list[str]) -> list:\n",
    "        self.vectorizer.run(sentences)\n",
    "        vectors = self.vectorizer.vectors[-len(sentences):]\n",
    "        distances = []\n",
    "        for vector in vectors:\n",
    "            distances.append([])\n",
    "            for vector2 in vectors:\n",
    "                distances[-1].append(spatial.distance.cosine(vector, vector2))\n",
    "        return distances\n",
    "\n",
    "\n",
    "distance_matrix = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "\n",
    "    vectorizer = SimilarityModel(\"sberbank-ai/sbert_large_nlu_ru\")\n",
    "    # vectorizer = SimilarityModel()\n",
    "    # vectorizer = SimilarityModel(\"distilbert-base-uncased\")\n",
    "\n",
    "    sentences = [\n",
    "        \"Рэпер Паша Техник впал в кому после отдыха на вписке с наркотиками\",\n",
    "        \"Рэпер Паша Техник впал в кому после вечеринки с наркотиками\",\n",
    "        \"Я не люблю кошек\",\n",
    "        \"Я не люблю собак\",\n",
    "        \"Я боюсь собак\",\n",
    "        \"Я боюсь Пашу Техника\",\n",
    "        \"Путин отмечает 70-летие\",\n",
    "        \"Владимир Путин отмечает юбилей: президенту России исполнилось 70 лет\",\n",
    "        \"Владимиру Путину — 70 лет! Топ-5 интересных фактов о жизни президента России\",\n",
    "        \"Владимиру Путину исполнилось 70 лет\",\n",
    "        \"День рождения Путина: президенту России исполнилось 70 лет\",\n",
    "    ]\n",
    "\n",
    "    from pprint import pprint\n",
    "    print(\"=\"*80)\n",
    "    pprint(vectorizer.calc_distance(sentences[0], sentences[1]))\n",
    "    print(\"=\"*80)\n",
    "    pprint(vectorizer.calc_distance_batch(sentences[0], sentences[1:]))\n",
    "    print(\"=\"*80)\n",
    "    distance_matrix = vectorizer.calc_distance_matrix(sentences)\n",
    "    # Convert to pandas df\n",
    "    df = pd.DataFrame(distance_matrix, columns=sentences, index=sentences)\n",
    "    # print matrix\n",
    "    for line in distance_matrix:\n",
    "        print(line)\n",
    "\n",
    "\n",
    "    pprint(df < 0.1)\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # vectors = vectorizer.vectors[-3:]\n",
    "    # dist_1 = spatial.distance.cosine(vectors[0], vectors[1])\n",
    "    # dist_2 = spatial.distance.cosine(vectors[0], vectors[2])\n",
    "    # print('dist_1: {0}, dist_2: {1}'.format(dist_1, dist_2))\n",
    "    # assert dist_1 < dist_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(distance_matrix, cmap='hot', interpolation='nearest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
